# Robust Realtime Data Integration and Streaming

## Table of Contents
- [Introduction](#introduction)
- [Architecture Overview](#architecture-overview)
- [Key Learning Objectives](#key-learning-objectives)
- [Technology Stack](#technology-stack)
- [Getting Started](#getting-started)

## Introduction

Explore the intricacies of constructing a comprehensive end-to-end data engineering pipeline through this project guide. The journey encompasses data ingestion, processing, and storage, leveraging a robust tech stack that includes Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, Cassandra, and Docker for seamless deployment and scalability.

## Architecture Overview

![System Architecture](https://github.com/aliyevom/E2E-DataStreaming/blob/main/Data%20engineering%20architecture.jpg?raw=true)

The project comprises the following key components:

- **Data Source**: Utilizes the `randomuser.me` API to generate random user data for the pipeline.
- **Apache Airflow**: Orchestrates the pipeline, storing fetched data in a PostgreSQL database.
- **Apache Kafka and Zookeeper**: Facilitates streaming data from PostgreSQL to the processing engine.
- **Control Center and Schema Registry**: Aids in monitoring and managing Kafka streams' schemas.
- **Apache Spark**: Employed for data processing, featuring master and worker nodes.
- **Cassandra**: Serves as the repository for the processed data.

## Key Learning Objectives

- Establishing a data pipeline using Apache Airflow
- Implementing real-time data streaming with Apache Kafka
- Enabling distributed synchronization with Apache Zookeeper
- Applying data processing techniques with Apache Spark
- Utilizing Cassandra and PostgreSQL for data storage
- Containerizing the entire data engineering setup using Docker

## Technology Stack

- Apache Airflow
- Python
- Apache Kafka
- Apache Zookeeper
- Apache Spark
- Cassandra
- PostgreSQL
- Docker

## Getting Started

1. Clone the repository:
    ```bash
    git clone 
    ```

2. Navigate to the project directory:
    ```bash
    cd E2E
    ```

3. Execute Docker Compose to initiate the services:
    ```bash
    docker-compose up
    ```
